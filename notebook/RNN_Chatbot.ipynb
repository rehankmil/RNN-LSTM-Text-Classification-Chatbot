{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuXfhl-WbUpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3fac38f4-16db-4595-a7e1-889bc2acd370",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1100616390.py, line 1)",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install tensorflow\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import json\n",
        "from google.colab import files\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Memicu jendela unggah file\n",
        "print(\"Silakan unggah file JSON Anda (misal: data_chatbot.json)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2. Mendapatkan nama file yang diunggah\n",
        "# Asumsi Anda hanya mengunggah SATU file JSON\n",
        "file_name = next(iter(uploaded))\n",
        "print(f\"File '{file_name}' berhasil diunggah.\")\n",
        "\n",
        "# 3. Memuat konten file\n",
        "# Konten file disimpan dalam bentuk bytes. Kita perlu membacanya.\n",
        "squad_data = json.load(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "# 4. Melanjutkan dengan ekstraksi data seperti sebelumnya\n",
        "input_texts = []  # Pertanyaan\n",
        "target_texts = [] # Jawaban\n",
        "\n",
        "for article in squad_data['data']:\n",
        "    for paragraph in article['paragraphs']:\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            if qa['answers']:\n",
        "                answer = qa['answers'][0]['text']\n",
        "                # Tambahkan token <start> dan <end>\n",
        "                formatted_answer = '<start> ' + answer + ' <end>'\n",
        "\n",
        "                input_texts.append(question)\n",
        "                target_texts.append(formatted_answer)\n",
        "\n",
        "print(f\"Total pasangan data (Q-A) yang diekstrak: {len(input_texts)}\")\n",
        "print(f\"Contoh Pertanyaan pertama: {input_texts[0]}\")"
      ],
      "metadata": {
        "id": "b_3_09ITwi3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Konfigurasi (Hemat RAM) ---\n",
        "MAX_WORDS = 7000     # Batasan Vocabulary\n",
        "MAX_LEN = 30         # Batasan Panjang Urutan\n",
        "EMBEDDING_DIM = 50\n",
        "RNN_UNITS = 128\n",
        "\n",
        "# 1. Inisialisasi Tokenizer (Asumsi input_texts dan target_texts sudah terisi)\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, filters='')\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "\n",
        "# 2. Konversi Teks ke Urutan Angka\n",
        "encoder_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "decoder_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# 3. Padding (Menyamakan panjang urutan)\n",
        "encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, padding='post')\n",
        "decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "# 4. MEMBUAT TARGET DECODER: Menggunakan Sparse Integer Targets (Solusi RAM)\n",
        "# Kita HANYA menyimpan indeks kata, bukan matriks One-Hot yang besar.\n",
        "# Ini adalah pengganti langsung untuk seluruh Tahap 4 Anda sebelumnya.\n",
        "\n",
        "# Buat array target integer dengan shape yang sama dengan input decoder\n",
        "decoder_target_data_sparse = np.zeros_like(decoder_input_data, dtype='int32')\n",
        "\n",
        "# Looping untuk Pergeseran Target (Shift)\n",
        "# Target di waktu 't' adalah kata yang ada di decoder_input_data di waktu 't+1'\n",
        "for i in range(decoder_input_data.shape[0]):\n",
        "    # Salin semua elemen dari indeks 1 hingga akhir (digeser 1 langkah)\n",
        "    # Target harus selalu lebih pendek 1 dari input\n",
        "    decoder_target_data_sparse[i, :-1] = decoder_input_data[i, 1:]\n",
        "\n",
        "# Ukuran Vocabulary (diperlukan untuk Dense layer)\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "print(\"\\n--- Hasil Preprocessing ---\")\n",
        "print(f\"Ukuran Vocabulary: {vocabulary_size}\")\n",
        "print(f\"Shape Encoder Input (Pertanyaan): {encoder_input_data.shape}\")\n",
        "print(f\"Shape Decoder Input (Jawaban Input): {decoder_input_data.shape}\")\n",
        "# Perhatikan shape target sekarang: tidak ada lagi dimensi vocabulary_size yang besar!\n",
        "print(f\"Shape Decoder Target (Sparse Integer): {decoder_target_data_sparse.shape}\")"
      ],
      "metadata": {
        "id": "_WwUaeCN-8h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.1. ENCODER ---\n",
        "encoder_inputs = Input(shape=(MAX_LEN,))\n",
        "encoder_embedding = Embedding(vocabulary_size, EMBEDDING_DIM)(encoder_inputs)\n",
        "\n",
        "# SimpleRNN Encoder: return_state=True untuk mendapatkan state terakhir (context vector)\n",
        "encoder_rnn = SimpleRNN(RNN_UNITS, return_state=True)\n",
        "encoder_outputs, state_h = encoder_rnn(encoder_embedding)\n",
        "encoder_states = state_h # Context vector dari input\n",
        "\n",
        "# --- 3.2. DECODER ---\n",
        "decoder_inputs = Input(shape=(MAX_LEN,))\n",
        "decoder_embedding = Embedding(vocabulary_size, EMBEDDING_DIM)(decoder_inputs)\n",
        "\n",
        "# SimpleRNN Decoder: return_sequences=True untuk output prediksi di setiap langkah waktu\n",
        "decoder_rnn = SimpleRNN(RNN_UNITS, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "# Dense Layer untuk memprediksi kata (softmax di atas vocabulary)\n",
        "decoder_dense = Dense(vocabulary_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# --- 3.3. TRAINING MODEL LENGKAP ---\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Kompilasi Model dengan Sparse Categorical Crossentropy\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\n--- Ringkasan Model Pelatihan ---\")\n",
        "model.summary()\n",
        "\n",
        "# --- 3.4. INFERENSI MODEL ---\n",
        "# Model Encoder Inferensi (Input: Pertanyaan -> Output: Context Vector)\n",
        "decoder_inputs_single = Input(shape=(1,), name='decoder_input_single')\n",
        "\n",
        "# Input State untuk Decoder Inferensi\n",
        "decoder_state_input = Input(shape=(RNN_UNITS,))\n",
        "\n",
        "# Decoder Inferensi (Input: 1 Token & Previous State -> Output: Prediksi 1 Token & New State)\n",
        "decoder_outputs, state_h_output = decoder_rnn(\n",
        "    decoder_embedding, initial_state=decoder_state_input\n",
        ")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs, decoder_state_input],\n",
        "    [decoder_outputs, state_h_output]\n",
        ")"
      ],
      "metadata": {
        "id": "8fiZHMvSBHBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "print(\"\\nMemulai Pelatihan Model Simple RNN...\")\n",
        "history = model.fit(\n",
        "    # Input Encoder (Pertanyaan) dan Input Decoder (Jawaban yang digeser)\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "\n",
        "    # Target (Jawaban yang digeser 1 langkah, dalam bentuk Sparse Integer)\n",
        "    decoder_target_data_sparse,\n",
        "\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=0.2 # Menggunakan 20% data untuk validasi\n",
        ")\n",
        "print(\"Pelatihan Selesai!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z5EbAuYEBXC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Asumsi: tokenizer, MAX_LEN, encoder_inputs, encoder_states, RNN_UNITS, decoder_rnn, decoder_dense sudah didefinisikan dari blok sebelumnya.\n",
        "\n",
        "# --- 1. DEFINISI MODEL INFERENSI (Wajib di sini untuk menghindari NameError) ---\n",
        "# KODE DI BAWAH INI MEMBUTUHKAN VARIABEL DARI BLOK MODELLING (rnn_seq2seq_model.py)\n",
        "\n",
        "# Model Encoder Inferensi (Input: Pertanyaan -> Output: Context Vector)\n",
        "# KODE INI MENGGUNAKAN LAYER DAN INPUT DARI BLOK MODELLING\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Input State untuk Decoder Inferensi\n",
        "decoder_state_input = Input(shape=(RNN_UNITS,), name='decoder_state_input')\n",
        "decoder_inputs_single = Input(shape=(1,), name='decoder_input_single')\n",
        "decoder_embedding_inference = encoder_model.get_layer(name='embedding')(decoder_inputs_single) # Asumsi layer embedding dinamai 'embedding_1'\n",
        "\n",
        "# Decoder Inferensi (Input: 1 Token & Previous State -> Output: Prediksi 1 Token & New State)\n",
        "decoder_outputs, state_h_output = decoder_rnn(\n",
        "    decoder_embedding_inference, initial_state=decoder_state_input\n",
        ")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single, decoder_state_input],\n",
        "    [decoder_outputs, state_h_output]\n",
        ")\n",
        "# --- AKHIR DEFINISI MODEL INFERENSI ---\n",
        "\n",
        "# Memetakan indeks token ke kata (untuk output)\n",
        "reverse_word_index = dict((i, word) for word, i in tokenizer.word_index.items())\n",
        "\n",
        "# --- 2. FUNGSI DECODING ---\n",
        "def decode_sequence(input_seq):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menghasilkan urutan jawaban dari urutan pertanyaan.\n",
        "    Menggunakan Model Inferensi (Encoder-Decoder) secara langkah demi langkah.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Encode input dan dapatkan initial state (context vector)\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # 2. Inisialisasi input target dengan token <start>\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    start_token_index = tokenizer.word_index.get('start')\n",
        "    if start_token_index is None:\n",
        "        print(\"Error: Token '<start>' tidak ditemukan di vocabulary.\")\n",
        "        return \"\"\n",
        "\n",
        "    target_seq[0, 0] = start_token_index\n",
        "\n",
        "    # 3. Looping Decoding Kata demi Kata\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    max_len_limit = MAX_LEN\n",
        "\n",
        "    while not stop_condition:\n",
        "        # Prediksi output (token berikutnya) dan state baru\n",
        "        output_tokens, h = decoder_model.predict([target_seq, states_value])\n",
        "\n",
        "        # Ambil token dengan probabilitas tertinggi (argmax)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "        sampled_word = reverse_word_index.get(sampled_token_index, '')\n",
        "\n",
        "        # Cek kondisi berhenti\n",
        "        if (sampled_word == 'end' or len(decoded_sentence.split()) >= max_len_limit):\n",
        "            stop_condition = True\n",
        "        elif sampled_word not in ['start', '']:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Perbarui input target (kata yang baru diprediksi menjadi input decoder berikutnya)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Perbarui state (h) untuk langkah waktu berikutnya\n",
        "        states_value = h\n",
        "\n",
        "    return decoded_sentence.strip()"
      ],
      "metadata": {
        "id": "FTBRKoqpBbZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perlu menginstal NLTK di Colab:\n",
        "!pip install nltk # Perintah paksa untuk memastikan semua dependensi terinstal\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import pad_sequences dari lokasi yang lebih stabil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "# --- PENTING: MENAMBAHKAN PATH DATA NLTK SECARA EKSPLISIT ---\n",
        "# Ini adalah solusi untuk mengatasi LookupError: Resource punkt_tab not found di Colab\n",
        "import nltk.data\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "\n",
        "# --- PENTING: PASTIKAN DOWNLOAD NLTK BERHASIL (Mengatasi punkt_tab error) ---\n",
        "try:\n",
        "    # Mengunduh secara eksplisit di awal dan menambahkan paket yang sering dibutuhkan\n",
        "    nltk.download('punkt', quiet=False)\n",
        "    nltk.download('wordnet', quiet=False)\n",
        "    # Download paket lain yang mungkin menggunakan punkt_tab secara internal\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=False)\n",
        "except Exception as e:\n",
        "    print(f\"Peringatan: Gagal mendownload NLTK. Evaluasi mungkin gagal. Error: {e}\")\n",
        "\n",
        "# --- ASUMSI VARIABEL GLOBAL SUDAH TERSEDIA ---\n",
        "# model, tokenizer, MAX_LEN, input_texts, target_texts, dll.\n",
        "\n",
        "# Asumsi fungsi decode_sequence dan variabel global lainnya sudah ada dari full_inference.py\n",
        "\n",
        "# Membagi data menjadi training dan testing (90% training, 10% testing)\n",
        "# Ini dilakukan agar evaluasi menggunakan data yang belum pernah dilihat model.\n",
        "try:\n",
        "    if 'input_texts' not in globals() or not input_texts:\n",
        "        raise NameError(\"Variabel input_texts belum dimuat.\")\n",
        "\n",
        "    # Memastikan data testing menggunakan input_texts, bukan input_texts_train\n",
        "    # Membagi 10% data untuk testing\n",
        "    input_train, input_test, target_train, target_test = train_test_split(\n",
        "        input_texts, target_texts, test_size=0.1, random_state=42\n",
        "    )\n",
        "\n",
        "    # Pre-process data testing\n",
        "    encoder_test_sequences = tokenizer.texts_to_sequences(input_test)\n",
        "    encoder_test_data = pad_sequences(encoder_test_sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "    print(f\"\\nTotal sampel untuk Evaluasi (Testing): {len(encoder_test_data)}\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}. Pastikan Anda menjalankan Preprocessing (Langkah 1) terlebih dahulu.\")\n",
        "    exit() # Menghentikan eksekusi jika data belum siap\n",
        "\n",
        "\n",
        "# --- FUNGSI BANTU UNTUK BLEU SCORE ---\n",
        "# SmoothingFunction untuk menangani kasus 0-matching (agar tidak error ZeroDivisionError)\n",
        "smoothie = SmoothingFunction().method1\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    \"\"\"Menghitung BLEU Score untuk satu pasangan jawaban.\"\"\"\n",
        "\n",
        "    # Tokenisasi Jawaban Asli (Referensi) dan Jawaban Model (Kandidat)\n",
        "    # Jawaban Asli harus berupa list of lists of tokens\n",
        "    reference_tokens = [nltk.word_tokenize(ref) for ref in reference.split(' | ')]\n",
        "    candidate_tokens = nltk.word_tokenize(candidate)\n",
        "\n",
        "    # Hitung BLEU Score\n",
        "    # Sentence BLEU digunakan untuk membandingkan satu kalimat\n",
        "    try:\n",
        "        score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothie)\n",
        "    except Exception:\n",
        "        # Menghandle error jika tokenization gagal pada sampel tertentu\n",
        "        return 0.0\n",
        "    return score\n",
        "\n",
        "# --- RUN EVALUASI DAN HITUNG BLEU SCORE RATA-RATA ---\n",
        "bleu_scores = []\n",
        "num_samples = len(encoder_test_data)\n",
        "# Gunakan hanya 100 sampel pertama agar tidak terlalu lama (tergantung ukuran data Anda)\n",
        "max_samples_to_test = min(num_samples, 5)\n",
        "\n",
        "print(f\"\\nMemulai evaluasi pada {max_samples_to_test} sampel...\")\n",
        "\n",
        "for i in range(max_samples_to_test):\n",
        "    # Ambil input (pertanyaan) dari data testing\n",
        "    input_seq = np.expand_dims(encoder_test_data[i], axis=0)\n",
        "\n",
        "    # Dapatkan Jawaban Model (prediksi)\n",
        "    predicted_answer_raw = decode_sequence(input_seq)\n",
        "\n",
        "    # Dapatkan Jawaban Asli (referensi)\n",
        "    # Hapus token <start> dan <end> dari target_test\n",
        "    reference_answer_raw = target_test[i].replace('<start>', '').replace('<end>', '').strip()\n",
        "\n",
        "    # Bersihkan Jawaban Model (hapus padding, karakter berulang, dll.)\n",
        "    predicted_answer = predicted_answer_raw.replace('<end>', '').strip()\n",
        "\n",
        "    # Tokenisasi dan Hitung BLEU\n",
        "    bleu_score_sample = calculate_bleu(reference_answer_raw, predicted_answer)\n",
        "    bleu_scores.append(bleu_score_sample)\n",
        "\n",
        "    # Tampilkan contoh setiap 20 sampel\n",
        "    if (i + 1) % 20 == 0 or i == max_samples_to_test - 1:\n",
        "        print(f\"\\n--- Contoh Sampel {i+1} ---\")\n",
        "        print(f\"Pertanyaan: {input_test[i]}\")\n",
        "        print(f\"Jawaban Model: {predicted_answer}\")\n",
        "        print(f\"Jawaban Asli: {reference_answer_raw}\")\n",
        "        print(f\"BLEU Score Sampel: {bleu_score_sample:.4f}\")\n",
        "\n",
        "# --- HASIL AKHIR ---\n",
        "final_bleu_score = np.mean(bleu_scores)\n",
        "\n",
        "print(\"\\n================================================\")\n",
        "print(f\"FINAL BLEU SCORE RATA-RATA ({max_samples_to_test} Sampel): {final_bleu_score:.4f}\")\n",
        "print(\"================================================\")\n"
      ],
      "metadata": {
        "id": "0fqbrLAk36Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mengekstrak data dari objek history\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "accuracy = history_dict['accuracy']\n",
        "val_accuracy = history_dict['val_accuracy']\n",
        "\n",
        "epochs_range = range(1, len(accuracy) + 1)\n",
        "\n",
        "# Membuat plot untuk Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, loss_values, 'bo', label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss_values, 'b', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Membuat plot untuk Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, accuracy, 'ro', label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dS5mxyHmSCG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}